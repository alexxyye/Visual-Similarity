install.packages("reshape2")
library(readr)
mean_df_res <- read_csv("Documents/GitHub/Visual-Similarity/analysis/mean_df_res.csv")
View(mean_df_res)
?grep
study1 <- mean_df_res %>% filter(grep("1#", ...1))
library(dplyr)
study1 <- mean_df_res %>% filter(grep("1#", ...1))
library(dplyr)
study1 <- mean_df_res %>% filter(grep("1$", ...1))
grep("1$", ...1)
library(readr)
mean_df_res <- read_csv("Documents/GitHub/Visual-Similarity/analysis/mean_df_res.csv", col_names[1] = "condition")
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(reshape2)
library(dplyr)
library(lsr)
library(ggplot2)
library(caret)
# set cd
setwd("/Users/alex/Documents/GitHub/Visual-Similarity/results/final2/")
filelist <- list.files("./", pattern = "*.csv")
n = 0
df.filelist <- list()
for (file_n in filelist){
n = n + 1
df <- read_csv(file_n, col_types = cols(file_loc = col_skip()))
df.filelist[[n]] <- df
}
split_into_2 <- function(df, loc, which_half){
if (which_half == 1){
df1 <- df[row.names(df) %in% 1:loc, ]
return(df1)
}
else if (which_half == 2) {
df2 <- df[row.names(df) %in% (loc+1):nrow(df), ]
return(df2)
}
else {
print("No Valid which_half input")
}
}
normalize_df <- function(df) {
# Find the maximum and minimum values for each column in the dataframe
max_vals <- apply(df, 2, max)
min_vals <- apply(df, 2, min)
# Subtract the minimum value from each column
df_sub <- sweep(df, 2, min_vals, `-`)
# Divide each column by its range
range_vals <- max_vals - min_vals
df_normalized <- sweep(df_sub, 2, range_vals, `/`)
return(df_normalized)
}
# split raw data by condition
symbol1 <- split_into_2(df.filelist[[1]], 60, 1)
german1 <- split_into_2(df.filelist[[1]], 60, 2)
symbol2 <- split_into_2(df.filelist[[2]], 60, 1)
english2 <- split_into_2(df.filelist[[2]], 60, 2)
symbol3 <- split_into_2(df.filelist[[3]], 59, 1)
english3 <-split_into_2(df.filelist[[3]], 59, 2)
falsefont4 <-split_into_2(df.filelist[[4]], 84, 1)
german4 <- split_into_2(df.filelist[[4]], 84, 2)
chinese5 <- split_into_2(df.filelist[[5]], 60, 1)
korean5 <- split_into_2(df.filelist[[5]], 60, 2)
chinese6 <- split_into_2(df.filelist[[6]], 40, 1)
korean6 <- split_into_2(df.filelist[[6]], 40, 2)
chinese7 <- split_into_2(df.filelist[[7]], 120, 1)
korean7 <- split_into_2(df.filelist[[7]], 120, 2)
# prepare a null mean list
mean_list <- data.frame(matrix(ncol = 6, nrow = 14))
colnames(mean_list) <- c("condition", "obj_num", "disc_strk", "strk_sum", "junc_sum", "PC")
mean_list$condition <- c('symbol1', 'german1', 'symbol2', 'english2', 'symbol3', 'english3',
'falsefont4', 'german4', 'chinese5', 'korean5',
'chinese6', 'korean6', 'chinese7', 'korean7')
# put all data together by condition
cond_list <- list(symbol1, german1, symbol2, english2, symbol3, english3,
falsefont4, german4, chinese5, korean5,
chinese6, korean6, chinese7, korean7)
for (cond_i in 1:14){
for (col_j in 2:6){
mean_ij <- mean(cond_list[[cond_i]][[col_j]])
mean_list[cond_i, col_j] <- mean_ij
}
}
# make a null normalized mean list
norm_mean_list <- data.frame(matrix(data = NA, nrow = 14, ncol = 6))
norm_mean_list[, 2:6] <- normalize_df(mean_list[ , 2:6])
colnames(norm_mean_list) <- c("condition", "obj_num", "disc_strk", "strk_sum", "junc_sum", "PC")
norm_mean_list$condition <- c('symbol1', 'german1', 'symbol2', 'english2', 'symbol3', 'english3',
'falsefont4', 'german4', 'chinese5', 'korean5',
'chinese6', 'korean6', 'chinese7', 'korean7')
norm_1 <- norm_mean_list
# prepare a null list holding normalizaed data for each study
norm_study_list <- list()
for (study_i in 1:7){
ori <- df.filelist[[study_i]][, 1]
norm_dat <- normalize_df(df.filelist[[study_i]][, 2:6])
norm_study_list[[study_i]] <- cbind(ori, norm_dat)
}
# split into conditions
symbol1_norm <- split_into_2(norm_study_list[[1]], 60, 1)
german1_norm <- split_into_2(norm_study_list[[1]], 60, 2)
symbol2_norm <- split_into_2(norm_study_list[[2]], 60, 1)
english2_norm <- split_into_2(norm_study_list[[2]], 60, 2)
symbol3_norm <- split_into_2(norm_study_list[[3]], 59, 1)
english3_norm <-split_into_2(norm_study_list[[3]], 59, 2)
falsefont4_norm <-split_into_2(norm_study_list[[4]], 84, 1)
german4_norm <- split_into_2(norm_study_list[[4]], 84, 2)
chinese5_norm <- split_into_2(norm_study_list[[5]], 60, 1)
korean5_norm <- split_into_2(norm_study_list[[5]], 60, 2)
chinese6_norm <- split_into_2(norm_study_list[[6]], 40, 1)
korean6_norm <- split_into_2(norm_study_list[[6]], 40, 2)
chinese7_norm <- split_into_2(norm_study_list[[7]], 120, 1)
korean7_norm <- split_into_2(norm_study_list[[7]], 120, 2)
# make a normalized list
norm_cond_list <- list(symbol1_norm, german1_norm, symbol2_norm, english2_norm, symbol3_norm, english3_norm, falsefont4_norm, german4_norm, chinese5_norm, korean5_norm, chinese6_norm, korean6_norm, chinese7_norm, korean7_norm)
norm_mean_list <- data.frame(matrix(data = NA, nrow = 14, ncol = 6))
colnames(norm_mean_list) <- c("condition", "obj_num", "disc_strk", "strk_sum", "junc_sum", "PC")
norm_mean_list$condition <- c('symbol1', 'german1', 'symbol2', 'english2', 'symbol3', 'english3',
'falsefont4', 'german4', 'chinese5', 'korean5',
'chinese6', 'korean6', 'chinese7', 'korean7')
# calculate mean
for (cond_i in 1:14){
for (col_j in 2:6){
mean_ij <- mean(norm_cond_list[[cond_i]][[col_j]])
norm_mean_list[cond_i, col_j] <- mean_ij
}
}
norm_2 <- norm_mean_list
# split raw data by condition
symbol1 <- split_into_2(df.filelist[[1]], 60, 1)
german1 <- split_into_2(df.filelist[[1]], 60, 2)
symbol2 <- split_into_2(df.filelist[[2]], 60, 1)
english2 <- split_into_2(df.filelist[[2]], 60, 2)
symbol3 <- split_into_2(df.filelist[[3]], 59, 1)
english3 <-split_into_2(df.filelist[[3]], 59, 2)
falsefont4 <-split_into_2(df.filelist[[4]], 84, 1)
german4 <- split_into_2(df.filelist[[4]], 84, 2)
chinese5 <- split_into_2(df.filelist[[5]], 60, 1)
korean5 <- split_into_2(df.filelist[[5]], 60, 2)
chinese6 <- split_into_2(df.filelist[[6]], 40, 1)
korean6 <- split_into_2(df.filelist[[6]], 40, 2)
chinese7 <- split_into_2(df.filelist[[7]], 120, 1)
korean7 <- split_into_2(df.filelist[[7]], 120, 2)
# put all data together by condition
cond_list <- list(symbol1, german1, symbol2, english2, symbol3, english3,
falsefont4, german4, chinese5, korean5,
chinese6, korean6, chinese7, korean7)
# make a null normalized mean list
norm_mean_list <- data.frame(matrix(data = NA, nrow = 14, ncol = 6))
colnames(norm_mean_list) <- c("condition", "obj_num", "disc_strk", "strk_sum", "junc_sum", "PC")
norm_mean_list$condition <- c('symbol1', 'german1', 'symbol2', 'english2', 'symbol3', 'english3',
'falsefont4', 'german4', 'chinese5', 'korean5',
'chinese6', 'korean6', 'chinese7', 'korean7')
for (cond_i in 1:14){
cond_raw_i <- (cond_list[[cond_i]])
a <- normalize_df(cond_raw_i[, 2:6])
b <- cond_raw_i[, 1]
norm_cond_i <- cbind(b,a)
for (col_j in 2:6){
mean_i <- mean(norm_cond_i[, col_j])
norm_mean_list[cond_i, col_j] <- mean_i
}
}
norm_3 <- norm_mean_list
study1 <- norm_1[1:2, ]
study2 <- norm_1[3:4, ]
study3 <- norm_1[5:6, ]
study4 <- norm_1[7:8, ]
study5 <- norm_1[9:10, ]
study6 <- norm_1[11:12, ]
study7 <- norm_1[13:14, ]
studylist <- list(study1, study2, study3, study4, study5, study6, study7)
n = 0
for (stdy_n in studylist){
n = n + 1
# stdy_n <- stdy_n %>% select(condition, stroke_sum, PC) # select columns you want
studylist[[n]] <- stdy_n}
# paired t-test
summary_list <- list()
df <- data.frame(matrix(ncol = 8, nrow = 7))
colnames(df) <- c("study.ID", "p.value", "dgr.of.frd", "conf.int.lb", "conf.int.ub", "mean.diff", "std.err", "cohens.d")
i = 0
for (studies in studylist){
i = i + 1
# wide to long
studies <- melt(studies, id.vars = "condition")
studies <- arrange(studies, condition) #sorting by condition
# paired t-test
t_test <- t.test(value ~ condition,
data = studies,
paired = TRUE)
# effect size
efs <- cohensD(value ~ condition,
data   = studies,
method = "paired")
summaries <- capture.output(print(t_test), print(efs))
summary_list[[i]] <- summaries
df[i, 1] <- i
df[i, 2] <- t_test$p.value
df[i, 3] <- t_test$parameter
df[i, 4] <- t_test$conf.int[1]
df[i, 5] <- t_test$conf.int[2]
df[i, 6] <- t_test$estimate
df[i, 7] <- t_test$stderr
df[i, 8] <- efs
}
df <- data.frame(matrix(ncol = 2, nrow = 7))
colnames(df) <- c("study.ID", "diff")
para_name <- '' # !!!!!CHANGE PARAMETER NAME HERE!!!!!
for (n in 1:7){
diff <- studylist[[n]][2, 2]-studylist[[n]][1, 2]
df[n, 1] <- n
df[n, 2] <- diff
}
library(meta)
library(metafor)
# import data
meta <- read_csv("/Users/alex/Documents/GitHub/Visual-Similarity/analysis/meta.csv")
colnames(meta)[30] <- "visual.similarity"
colnames(meta)[31] <- "writing.system"
# replace Visual Similarity Index with results from VS analysis
meta[30] <- rev(df$cohens.d)
# meta[30] <- rev(df$diff)
# call the metacont random effect model
mod = metacont(
n.early,
mean.early.E64,
sd.early.E64,
n.late,
mean.late.E64,
sd.late.E64,
data = meta,
studlab = meta$author,
method.smd = "Hedges",
method.bias = "Egger")
MetaReg.vs <- metareg(mod, formula = ~ visual.similarity, method.tau = "REML", hakn = TRUE)
# import data
meta <- read_csv("/Users/alex/Documents/GitHub/Visual-Similarity/analysis/meta.csv")
colnames(meta)[30] <- "visual.similarity"
colnames(meta)[31] <- "writing.system"
# replace Visual Similarity Index with results from VS analysis
meta[30] <- rev(df$cohens.d)
# meta[30] <- rev(df$diff)
# call the metacont random effect model
mod = metacont(
n.early,
mean.early.E64,
sd.early.E64,
n.late,
mean.late.E64,
sd.late.E64,
data = meta,
studlab = meta$author,
method.smd = "Hedges",
method.bias = "Egger")
MetaReg.vs <- metareg(mod, formula = ~ visual.similarity, method.tau = "REML", hakn = TRUE)
# import data
meta <- read_csv("/Users/alex/Documents/GitHub/Visual-Similarity/analysis/meta.csv")
colnames(meta)[30] <- "visual.similarity"
colnames(meta)[31] <- "writing.system"
# replace Visual Similarity Index with results from VS analysis
meta[30] <- rev(df$cohens.d)
# meta[30] <- rev(df$diff)
# call the metacont random effect model
mod = metacont(
n.early,
mean.early.E64,
sd.early.E64,
n.late,
mean.late.E64,
sd.late.E64,
data = meta,
studlab = meta$author,
method.smd = "Hedges",
method.bias = "Egger")
MetaReg.vs <- metareg(mod, formula = ~ visual.similarity, method.tau = "REML", hakn = TRUE)
mod
MetaReg.ws <- metareg(mod, formula = ~ writing.system, method.tau = "REML", hakn = TRUE)
View(meta)
# import data
meta <- read_csv("/Users/alex/Documents/GitHub/Visual-Similarity/analysis/meta.csv")
colnames(meta)[30] <- "visual.similarity"
colnames(meta)[31] <- "writing.system"
# replace Visual Similarity Index with results from VS analysis
meta[30] <- rev(df$cohens.d)
# meta[30] <- rev(df$diff)
# call the metacont random effect model
mod = metacont(
n.early,
mean.early.E64,
sd.early.E64,
n.late,
mean.late.E64,
sd.late.E64,
data = meta,
studlab = meta$author,
method.smd = "Hedges",
method.bias = "Egger")
View(df)
# import data
meta <- read_csv("/Users/alex/Documents/GitHub/Visual-Similarity/analysis/meta.csv")
colnames(meta)[30] <- "visual.similarity"
colnames(meta)[31] <- "writing.system"
# replace Visual Similarity Index with results from VS analysis
# meta[30] <- rev(df$cohens.d)
meta[30] <- rev(df$diff)
# call the metacont random effect model
mod = metacont(
n.early,
mean.early.E64,
sd.early.E64,
n.late,
mean.late.E64,
sd.late.E64,
data = meta,
studlab = meta$author,
method.smd = "Hedges",
method.bias = "Egger")
MetaReg.vs <- metareg(mod, formula = ~ visual.similarity, method.tau = "REML", hakn = TRUE)
MetaReg.ws <- metareg(mod, formula = ~ writing.system, method.tau = "REML", hakn = TRUE)
MetaReg.vs
study_effects <- mod$TE
sample_sizes <- mod$n.e
study_labels <- mod$studlab
study_variances <- mod$seTE
fit <- rma(yi = study_effects, vi = study_variances)
# Create a data frame with the study effects, sample sizes, and study labels
df <- data.frame(study_effects, sample_sizes, study_labels)
# Predict the expected study effect for each sample size
pred <- predictInterval(df, level=0.95, digits=3)
library(meta)
?predictInterval
install.packages("merTools")
library(merTools)
study_effects <- mod$TE
sample_sizes <- mod$n.e
study_labels <- mod$studlab
study_variances <- mod$seTE
fit <- rma(yi = study_effects, vi = study_variances)
# Create a data frame with the study effects, sample sizes, and study labels
df <- data.frame(study_effects, sample_sizes, study_labels)
# Predict the expected study effect for each sample size
pred <- predictInterval(df, level=0.95, digits=3)
study_effects <- mod$TE
sample_sizes <- mod$n.e
study_labels <- mod$studlab
study_variances <- mod$seTE
fit <- rma(yi = study_effects, vi = study_variances)
# Create a data frame with the study effects, sample sizes, and study labels
df <- data.frame(study_effects, sample_sizes, study_labels)
# Predict the expected study effect for each sample size
pred <- predictInterval(df, level=0.95)
?predict
study_effects <- mod$TE
sample_sizes <- mod$n.e
study_labels <- mod$studlab
study_variances <- mod$seTE
fit <- rma(yi = study_effects, vi = study_variances)
# Calculate the regression line and confidence intervals
x_vals <- seq(min(sample_sizes), max(sample_sizes), length.out = 100)
preds <- predict(fit, newmods = log(x_vals))
study_effects <- mod$TE
sample_sizes <- mod$n.e
study_labels <- mod$studlab
study_variances <- mod$seTE
fit <- rma(yi = study_effects, vi = study_variances)
# Predict the fitted values
x_vals <- seq(min(sample_sizes), max(sample_sizes), length.out = 100)
y_vals <- predict(fit, newx = x_vals, level = 0.95, type = "response")
# Plot the results
ggplot(data.frame(study_effects, sample_sizes, study_labels), aes(x = sample_sizes, y = study_effects)) +
geom_point(aes(size = exp(fit$coef[1] + fit$coef[2] * log(sample_sizes))),
shape = 21, fill = "blue", alpha = 0.7) +
geom_text(aes(label = study_labels), check_overlap = TRUE, vjust = 1.5, hjust = 1.5) +
geom_smooth(aes(x = x_vals, y = y_vals$pred), se = TRUE, colour = "red", method = "lm") +
xlab("Sample Size") +
ylab("Study Effects") +
ggtitle("Meta-Regression Analysis") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
length(x_vals)
length(study_effects)
x_vals <- seq(min(sample_sizes), max(sample_sizes), length.out = 7)
length(study_effects)
length(x_vals)
study_effects <- mod$TE
sample_sizes <- mod$n.e
study_labels <- mod$studlab
study_variances <- mod$seTE
fit <- rma(yi = study_effects, vi = study_variances)
# Predict the fitted values
x_vals <- seq(min(sample_sizes), max(sample_sizes), length.out = 7)
y_vals <- predict(fit, newx = x_vals, level = 0.95, type = "response")
# Plot the results
ggplot(data.frame(study_effects, sample_sizes, study_labels), aes(x = sample_sizes, y = study_effects)) +
geom_point(aes(size = exp(fit$coef[1] + fit$coef[2] * log(sample_sizes))),
shape = 21, fill = "blue", alpha = 0.7) +
geom_text(aes(label = study_labels), check_overlap = TRUE, vjust = 1.5, hjust = 1.5) +
geom_smooth(aes(x = x_vals, y = y_vals$pred), se = TRUE, colour = "red", method = "lm") +
xlab("Sample Size") +
ylab("Study Effects") +
ggtitle("Meta-Regression Analysis") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
#
# ggplot(df, aes(x = study_effects, y = sample_sizes, label = study_labels)) +
#   geom_point(shape = 21, fill = "blue", alpha = 0.7) +
#   geom_text(check_overlap = TRUE, vjust = 1.5, hjust = 1.5) +
#   geom_smooth(method = "lm", se = FALSE, color = "red") +
#   xlab("Study Effects") +
#   ylab("Sample Size") +
#   ggtitle("Bubble Plot with Regression Line") +
#   theme_bw() +
#   theme(plot.title = element_text(hjust = 0.5))
study_effects <- mod$TE
sample_sizes <- mod$n.e
study_labels <- mod$studlab
study_variances <- mod$seTE
fit <- rma(yi = study_effects, vi = study_variances)
# Predict the fitted values
x_vals <- seq(min(sample_sizes), max(sample_sizes), length.out = 7)
y_vals <- predict(fit, newx = x_vals, level = 0.95, type = "response")
# Plot the results
ggplot(data.frame(study_effects, sample_sizes, study_labels), aes(x = sample_sizes, y = study_effects)) +
geom_point(aes(size = exp(fit$coef[1] + fit$coef[2] * log(sample_sizes))),
shape = 21, fill = "blue", alpha = 0.7) +
geom_text(aes(label = study_labels), check_overlap = TRUE, vjust = 1.5, hjust = 1.5) +
# geom_smooth(aes(x = x_vals, y = y_vals$pred), se = TRUE, colour = "red", method = "lm") +
xlab("Sample Size") +
ylab("Study Effects") +
ggtitle("Meta-Regression Analysis") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
#
# ggplot(df, aes(x = study_effects, y = sample_sizes, label = study_labels)) +
#   geom_point(shape = 21, fill = "blue", alpha = 0.7) +
#   geom_text(check_overlap = TRUE, vjust = 1.5, hjust = 1.5) +
#   geom_smooth(method = "lm", se = FALSE, color = "red") +
#   xlab("Study Effects") +
#   ylab("Sample Size") +
#   ggtitle("Bubble Plot with Regression Line") +
#   theme_bw() +
#   theme(plot.title = element_text(hjust = 0.5))
study_effects <- mod$TE
sample_sizes <- mod$n.e
study_labels <- mod$studlab
study_variances <- mod$seTE
fit <- rma(yi = study_effects, vi = study_variances)
# Predict the fitted values
x_vals <- seq(min(sample_sizes), max(sample_sizes), length.out = 7)
y_vals <- predict(fit, newx = x_vals, level = 0.95, type = "response")
# # Plot the results
# ggplot(data.frame(study_effects, sample_sizes, study_labels), aes(x = sample_sizes, y = study_effects)) +
#   geom_point(aes(size = exp(fit$coef[1] + fit$coef[2] * log(sample_sizes))),
#              shape = 21, fill = "blue", alpha = 0.7) +
#   geom_text(aes(label = study_labels), check_overlap = TRUE, vjust = 1.5, hjust = 1.5) +
#   geom_smooth(aes(x = x_vals, y = y_vals$pred), se = TRUE, colour = "red", method = "lm") +
#   xlab("Sample Size") +
#   ylab("Study Effects") +
#   ggtitle("Meta-Regression Analysis") +
#   theme_bw() +
#   theme(plot.title = element_text(hjust = 0.5))
#
ggplot(df, aes(x = study_effects, y = sample_sizes, label = study_labels)) +
geom_point(shape = 21, fill = "blue", alpha = 0.7) +
geom_text(check_overlap = TRUE, vjust = 1.5, hjust = 1.5) +
geom_smooth(method = "lm", se = FALSE, color = "red") +
xlab("Study Effects") +
ylab("Sample Size") +
ggtitle("Bubble Plot with Regression Line") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
knit_with_parameters("~/Documents/GitHub/Visual-Similarity/analysis/VS+META.Rmd")
setwd("/Users/alex/Documents/GitHub/Visual-Similarity")
ls(./)
ls()
ls(list="./")
source("~/Documents/GitHub/Visual-Similarity/EEG data/EEG_analysis.R", echo=TRUE)
temp = list.files(pattern="./EEG data/*.csv")
myfiles = lapply(temp, read.delim)
library(tidyverse)
setwd("/Users/alex/Documents/GitHub/Visual-Similarity/EEG data")
temp = list.files(pattern="*.csv")
myfiles = lapply(temp, read.delim)
View(myfiles)
myfiles[[1]]
myfiles = lapply(temp, read.delim)
for (file in myfiles) {
file_name <- gsub(".csv", "", file) # Remove the .csv extension from the filename
assign(file_name, read.csv(file)) # Read the file and assign it to a variable with the filename
}
for (file in temp) {
file_name <- gsub(".csv", "", file)
assign(file_name, read.csv(file))
}
source("~/Documents/GitHub/Visual-Similarity/EEG data/EEG_analysis.R", echo=TRUE)
View(1_offset_symbol)
View(1_onset_symbol)
for (file in temp) {
file_name <- gsub(".csv", "", file)
file_name <- gsub("_", "", file_name)
assign(file_name, read.csv(file))
}
source("~/Documents/GitHub/Visual-Similarity/EEG data/EEG_analysis.R", echo=TRUE)
View(1onsetword)
source("~/Documents/GitHub/Visual-Similarity/EEG data/EEG_analysis.R", echo=TRUE)
View(2_onset_word)
source("~/Documents/GitHub/Visual-Similarity/EEG data/EEG_analysis.R", echo=TRUE)
View(offset_kor_7)
View(offset_symbol_4)
